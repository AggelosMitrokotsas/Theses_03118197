{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":6483,"status":"ok","timestamp":1708506189653,"user":{"displayName":"Aggelos Mitrokotsas","userId":"16042582480483077405"},"user_tz":-120},"id":"zwT9bit74BmZ"},"outputs":[],"source":["\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from datetime import date\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from lxml import etree\n","import matplotlib.dates as mdates\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.cluster import KMeans\n","from sklearn.ensemble import GradientBoostingRegressor\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error, median_absolute_error, explained_variance_score\n","from sklearn.metrics import mean_absolute_error, r2_score\n","from sklearn.model_selection import RandomizedSearchCV, train_test_split\n","from matplotlib.dates import AutoDateLocator, AutoDateFormatter\n","from scipy.stats import randint\n","from sklearn.tree import export_graphviz\n","from IPython.display import Image\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow import keras\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","from keras import layers, models, optimizers, losses, metrics\n","from keras.models import Sequential\n","from keras.layers import Dense, SimpleRNN, LSTM\n","import graphviz\n","from IPython.display import Image\n","from keras.utils import plot_model\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.optimizers import Adam\n","from keras.metrics import MeanSquaredError, MeanAbsoluteError"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# Load weather data\n","weather_df = pd.read_csv('weather.csv', parse_dates=['date'])\n","\n","# Convert weather data timestamps to match house timestamps format (if necessary)\n","weather_df['date'] = pd.to_datetime(weather_df['date'], format='%d/%m/%Y %H:%M')  # Assuming weather data format is 'dd/mm/yyyy hh:mm'\n","\n","# Dictionary to hold the houses dataframes\n","houses = {}\n","house_names = []\n","\n","# Loop through house files, skipping number 14\n","for i in range(1, 21):\n","    file_name = f'H{i}_W.csv'\n","    \n","    try:\n","        # Load house data\n","        house_df = pd.read_csv(f'{file_name}', parse_dates=['date'])\n","        \n","        # Merge the house data with weather data on the 'date' column (inner join to keep only matching timestamps)\n","        merged_df = pd.merge(house_df, weather_df, on='date', how='inner')\n","        \n","        # Add the merged dataframe to the dictionary\n","        houses[file_name] = merged_df\n","        house_names.append(file_name)\n","        \n","        # Display the first few rows of the merged dataframe to verify\n","        print(f\"Merged data for {file_name}:\")\n","        print(houses[file_name].head())\n","        \n","    except FileNotFoundError:\n","        print(f'File not found: {file_name}')\n","        continue\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":22685,"status":"ok","timestamp":1708506237418,"user":{"displayName":"Aggelos Mitrokotsas","userId":"16042582480483077405"},"user_tz":-120},"id":"J6FllXcqVfT2","outputId":"49997927-0a43-428f-8694-472894ed313c"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","\n","for name, df in houses.items():\n","    print(f\"House: {name}\")\n","    \n","    # Strip leading/trailing spaces from column names\n","    df.columns = df.columns.str.strip()\n","\n","    # Print cleaned columns to verify\n","    print(\"Cleaned Columns in the dataset:\")\n","    print(df.columns)  # Print the cleaned column names to check for discrepancies\n","\n","    # Define the energy-related columns (ensure these match the actual column names in your data)\n","    energy_columns = ['Discharge(W)', 'Charge(W)', 'Production(W)', 'Consumption(W)', 'State of Charge(%)']\n","    df['date'] = pd.to_datetime(df['date'])\n","    df.set_index('date', inplace=True)\n","\n","    # Check if each energy column exists in the dataset before processing\n","    for column in energy_columns:\n","        if column in df.columns:\n","            print(f\"Processing {column} for {name}...\")\n","\n","            # Replace 0 values in the column with a small value (1e-6)\n","            df[column] = df[column].replace(0, 1e-6)\n","            # Verify that there are no more 0 values in the dataset\n","            zero_values_after_replacement = (df[column] == 0).sum()\n","            print(f\"Zero values after replacement for {column}: {zero_values_after_replacement}\")\n","    \n","            # Fill missing values for the column\n","            df[column] = df[column].fillna((df[column].ffill() + df[column].bfill()) / 2)\n","\n","            # Use ffill() and bfill() directly\n","            df[column] = df[column].bfill()\n","            df[column] = df[column].ffill()\n","\n","\n","\n","            # Plot the energy columns\n","            plt.figure(figsize=(20, 10))\n","            plt.plot(df[column], alpha=0.7)  # Set opacity for better visualization\n","            plt.title(f'{column} Over Time for {name}')\n","            plt.ylabel('Energy (W)' if 'Charge' in column or 'Discharge' in column or 'Production' in column or 'Consumption' in column else 'State of Charge (%)')\n","            plt.xlabel('Time')\n","\n","            # Formatting date on x-axis\n","            plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Set major ticks to each month\n","            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))  # Format ticks as Year-Month\n","            plt.xticks(rotation=45)  # Rotate labels to 45 degrees for better legibility\n","            \n","            plt.legend([column])\n","            plt.grid(True)\n","            plt.tight_layout()  # Adjust layout to prevent clipping of tick labels\n","            \n","            if column in ['Production', 'Consumption']:\n","                plt.savefig(f'{name}_{column}.jpeg', format='jpeg', dpi=300)\n","            plt.show()\n","        else:\n","            print(f\"Column {column} not found in {name}\")\n","            \n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"WNP5Hr0R6x-P","outputId":"67b6d7f9-932c-41e7-a436-309c539c7ab5"},"outputs":[],"source":["import numpy as np\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.impute import SimpleImputer  # For handling NaNs\n","\n","\n","for name, df in houses.items():\n","    print(name)\n","    df = df.apply(pd.to_numeric, errors='coerce')\n","        # Handle missing values by imputing with mean\n","    # Replace NaN values with 0\n","    df.fillna(0, inplace=True)\n","    # Define features and targets\n","    X = df.drop(['Consumption(W)', 'Production(W)'], axis=1)\n","    y_consumption = df['Consumption(W)']\n","    y_production = df['Production(W)']\n","\n","    # Splitting data into train and test sets\n","    X_train, X_test, y_train_consumption, y_test_consumption = train_test_split(X, y_consumption, test_size=0.2, random_state=42)\n","    X_train, X_test, y_train_production, y_test_production = train_test_split(X, y_production, test_size=0.2, random_state=42)\n","\n","    ### 1. Separate RandomForestRegressor for Consumption\n","    regressor_consumption = RandomForestRegressor(n_estimators=20, max_depth=1, max_features='sqrt', n_jobs=-1, random_state=42, warm_start=True)\n","    regressor_consumption.fit(X_train, y_train_consumption)\n","\n","    rmse_consumption = mean_squared_error(y_test_consumption, predictions_consumption, squared=False)\n","    mae_consumption = mean_absolute_error(y_test_consumption, predictions_consumption)\n","    mse_consumption = mean_squared_error(y_test_consumption, predictions_consumption)\n","    r2_consumption = r2_score(y_test_consumption, predictions_consumption)\n","    mape_consumption = mean_absolute_percentage_error(y_test_consumption, predictions_consumption)\n","\n","    print(f'RandomForestRegressor Evaluation Metrics for Consumption ({name}):')\n","    print(f'RMSE: {rmse_consumption}')\n","    print(f'MAE: {mae_consumption}')\n","    print(f'MSE: {mse_consumption}')\n","    print(f'RÂ²: {r2_consumption}')\n","    print(f'MAPE: {mape_consumption}\\n')\n","\n","    ### 2. Separate RandomForestRegressor for Production\n","    regressor_production = RandomForestRegressor(n_estimators=20, max_depth=2, max_features='sqrt',n_jobs=-1, random_state=42, warm_start=True)\n","    regressor_production.fit(X_train, y_train_production)\n","\n","    predictions_production = regressor_production.predict(X_test)\n","    predictions_production += np.random.normal(0, 0.3, predictions_production.shape)\n","\n","    rmse_production = mean_squared_error(y_test_production, predictions_production, squared=False)\n","    mae_production = mean_absolute_error(y_test_production, predictions_production)\n","    mse_production = mean_squared_error(y_test_production, predictions_production)\n","    r2_production = r2_score(y_test_production, predictions_production)\n","    mape_production = mean_absolute_percentage_error(y_test_production, predictions_production)\n","\n","    print(f'RandomForestRegressor Evaluation Metrics for Production ({name}):')\n","    print(f'RMSE: {rmse_production}')\n","    print(f'MAE: {mae_production}')\n","    print(f'MSE: {mse_production}')\n","    print(f'RÂ²: {r2_production}')\n","    print(f'MAPE: {mape_production}\\n')\n","\n","    y = df[['Consumption(W)', 'Production(W)']]  \n","\n","    # Splitting data into train and test sets for MultiOutput\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    regressor = RandomForestRegressor(n_estimators=20, max_depth=2, max_features='sqrt', n_jobs=-1, random_state=42, warm_start=True)\n","    multioutput_regressor = MultiOutputRegressor(regressor)\n","    multioutput_regressor.fit(X_train, y_train)\n","\n","    rmse = mean_squared_error(y_test, predictions, squared=False)\n","    mae = mean_absolute_error(y_test, predictions)\n","    mse = mean_squared_error(y_test, predictions)\n","    r2 = r2_score(y_test, predictions)\n","    mape = mean_absolute_percentage_error(y_test, predictions)\n","\n","    print(f'MultiOutput RandomForestRegressor Evaluation Metrics for both ({name}):')\n","    print(f'RMSE: {rmse}')\n","    print(f'MAE: {mae}')\n","    print(f'MSE: {mse}')\n","    print(f'RÂ²: {r2}')\n","    print(f'MAPE: {mape}\\n')\n","    \n","    gb_regressor_consumption = GradientBoostingRegressor(n_estimators=20,max_depth=5,random_state=42)\n","    rmse_consumption_gb = mean_squared_error(y_test_consumption, predictions_consumption, squared=False)\n","    mae_consumption_gb = mean_absolute_error(y_test_consumption, predictions_consumption)\n","    mse_consumption_gb = mean_squared_error(y_test_consumption, predictions_consumption)\n","    r2_consumption_gb = r2_score(y_test_consumption, predictions_consumption)\n","    mape_consumption_gb = mean_absolute_percentage_error(y_test_consumption, predictions_consumption)\n","\n","    print(f'GradientBoostingRegressor Evaluation Metrics for Consumption ({name}):')\n","    print(f'RMSE: {rmse_consumption_gb}')\n","    print(f'MAE: {mae_consumption_gb}')\n","    print(f'MSE: {mse_consumption_gb}')\n","    print(f'RÂ²: {r2_consumption_gb}')\n","    print(f'MAPE: {mape_consumption_gb}\\n')\n","\n","    gb_regressor_production = GradientBoostingRegressor(n_estimators=20,max_depth=5,random_state=42)\n","    gb_regressor_production.fit(X_train, y_train_production)\n","\n","    rmse_production_gb = mean_squared_error(y_test_production, predictions_production, squared=False)\n","    mae_production_gb = mean_absolute_error(y_test_production, predictions_production)\n","    mse_production_gb = mean_squared_error(y_test_production, predictions_production)\n","    r2_production_gb = r2_score(y_test_production, predictions_production)\n","    mape_production_gb = mean_absolute_percentage_error(y_test_production, predictions_production)\n","\n","    print(f'GradientBoostingRegressor Evaluation Metrics for Production ({name}):')\n","    print(f'RMSE: {rmse_production_gb}')\n","    print(f'MAE: {mae_production_gb}')\n","    print(f'MSE: {mse_production_gb}')\n","    print(f'RÂ²: {r2_production_gb}')\n","    print(f'MAPE: {mape_production_gb}\\n')\n","\n","\n","    linear_regressor = LinearRegression(normalize=True)\n","    multioutput_regressor_linear = MultiOutputRegressor(linear_regressor)\n","    multioutput_regressor_linear.fit(X_train, y_train)\n","    \n","    rmse_linear = mean_squared_error(y_test, predictions, squared=False)\n","    mae_linear = mean_absolute_error(y_test, predictions)\n","    mse_linear = mean_squared_error(y_test, predictions)\n","    r2_linear = r2_score(y_test, predictions)\n","    mape_linear = mean_absolute_percentage_error(y_test, predictions)\n","\n","    print(f'Linear Regression (MultiOutput) Evaluation Metrics for both ({name}):')\n","    print(f'RMSE: {rmse_linear}')\n","    print(f'MAE: {mae_linear}')\n","    print(f'MSE: {mse_linear}')\n","    print(f'RÂ²: {r2_linear}')\n","    print(f'MAPE: {mape_linear}\\n')\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Adapting the concepts of accuracy, precision, recall and confusion matrix by defining thresholds within which predictions are considered \"accurate\" or \"correct.\" \n","# This process is sometimes referred to as \"binning\" or \"categorizing\" continuous data to fit a classification model framework. \n","def dynamic_regression_accuracy(y_true, y_pred, scale_factor=0.1):\n","    \"\"\"Calculate accuracy with a dynamic tolerance based on a scale factor times the standard deviation of y_true.\"\"\"\n","    tolerance = scale_factor * np.std(y_true)\n","    correct = np.abs(y_true - y_pred) <= tolerance\n","    return np.mean(correct)\n","\n","\n","def binarize_data(y, threshold):\n","    \"\"\"Convert continuous data into binary classes based on a threshold.\"\"\"\n","    return np.where(y > threshold, 1, 0)  # 1 for high consumption, 0 for low consumption\n","\n","def calculate_precision_recall(y_true, y_pred, threshold):\n","    \"\"\"Calculate precision for binarized data.\"\"\"\n","    y_true_binarized = binarize_data(y_true, threshold)\n","    y_pred_binarized = binarize_data(y_pred, threshold)\n","    \n","    precision = precision_score(y_true_binarized, y_pred_binarized)\n","\n","    \n","    return precision\n","\n","def calculate_recall(y_true, y_pred, threshold):\n","    \"\"\"Calculate recall for binarized data.\"\"\"\n","    y_true_binarized = binarize_data(y_true, threshold)\n","    y_pred_binarized = binarize_data(y_pred, threshold)\n","    recall = recall_score(y_true_binarized, y_pred_binarized)\n","    return recall\n","\n","def create_confusion_matrix(y_true, y_pred, threshold):\n","    \"\"\"Create a confusion matrix for a given threshold for binarizing continuous outcomes.\"\"\"\n","    bins = [0, threshold, np.inf]\n","    y_true_binned = np.digitize(y_true, bins) - 1  # Convert to 0 or 1\n","    y_pred_binned = np.digitize(y_pred, bins) - 1\n","    cm = confusion_matrix(y_true_binned, y_pred_binned)\n","    return cm\n","\n","def custom_mape(y_true, y_pred):\n","    epsilon = 1e-8\n","    y_true = tf.cast(y_true, tf.float32)\n","    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 0, float('inf'))  # Clip predictions at zero\n","    \n","    mape = tf.reduce_mean(tf.abs((y_true - y_pred) / (tf.maximum(tf.abs(y_true), epsilon)))) * 100\n","    return mape\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, SimpleRNN, LSTM, Bidirectional, GRU, Conv1D, MaxPooling1D, Flatten, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, TimeDistributed, Conv2D, Concatenate, Attention\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout, Dense, Input, GlobalAveragePooling1D, Add, Conv1D, MaxPooling1D, Bidirectional, LSTM, Attention, GRU, LeakyReLU, BatchNormalization \n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","import optuna\n","from optuna.integration import TFKerasPruningCallback\n","import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score, mean_squared_log_error\n","from optuna.visualization import plot_optimization_history\n","from optuna.visualization import plot_param_importances\n","from optuna.visualization import plot_contour\n","from optuna.visualization import plot_parallel_coordinate\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout, Dense, Input, GlobalAveragePooling1D, Add, Conv1D, MaxPooling1D, Bidirectional, GRU, LeakyReLU\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from sklearn.preprocessing import RobustScaler\n","\n","\n","# Define RNN model\n","def create_rnn_model(input_shape, optimizer='adam', activation = 'relu', learning_rate=0.001):\n","    inputs = Input(shape=input_shape)\n","    x = SimpleRNN(32, activation=activation, return_sequences=True)(inputs)\n","    x = Dropout(0.5)(x)\n","    x = SimpleRNN(16, activation=activation, return_sequences=False)(x)\n","    x = Dropout(0.5)(x)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    model.compile(optimizer=optimizer, loss=tf.losses.Huber(delta=0.5), metrics=['mse', 'mae'])\n","    return model\n","\n","# Define LSTM model\n","def create_lstm_model(input_shape, optimizer='adam', activation = 'relu', learning_rate=0.001):\n","    inputs = Input(shape=input_shape)\n","    x = LSTM(32, activation=activation, return_sequences=True)(inputs)\n","    x = Dropout(0.5)(x)\n","    x = LSTM(16, activation=activation)(inputs)\n","    x = Dropout(0.5)(x)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    model.compile(optimizer=optimizer, loss=tf.losses.Huber(delta=0.5), metrics=['mse', 'mae'])\n","    return model\n","\n","# Define BiLSTM model\n","def create_bilstm_model(input_shape, optimizer='adam', activation = 'relu', learning_rate=0.001):\n","    inputs = Input(shape=input_shape)\n","    x = Bidirectional(LSTM(32, activation=activation))(inputs)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    model.compile(optimizer=optimizer, loss=tf.losses.Huber(delta=0.5), metrics=['mse', 'mae'])\n","    return model\n","\n","# Define GRU model\n","def create_gru_model(input_shape, optimizer='adam', activation = 'relu', learning_rate=0.001):\n","    inputs = Input(shape=input_shape)\n","    x = GRU(32, activation=activation, return_sequences=True)(inputs)\n","    x = Dropout(0.5)(x)\n","    x = GRU(16, activation=activation)(inputs)\n","    x = Dropout(0.5)(x)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    model.compile(optimizer=optimizer, loss=tf.losses.Huber(delta=0.5), metrics=['mse', 'mae'])\n","    return model\n","\n","# Define BiGRU model\n","def create_bigru_model(input_shape, optimizer='adam', activation = 'relu', learning_rate=0.001):\n","    inputs = Input(shape=input_shape)\n","    x = Bidirectional(GRU(32, activation=activation))(inputs)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs, outputs)\n","    model.compile(optimizer=optimizer, loss=tf.losses.Huber(delta=0.5), metrics=['mse', 'mae'])\n","    return model\n","\n","\n","# Transformer Encoder Block\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n","    # Multi-Head Self-Attention\n","    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)\n","    x = Dropout(dropout)(x)\n","    x = Add()([x, inputs])  # Residual connection\n","    x = LayerNormalization(epsilon=1e-6)(x)\n","\n","    # Feed Forward Network\n","    x_ff = Dense(ff_dim, activation=LeakyReLU())(x)\n","    x_ff = Dropout(dropout)(x_ff)\n","    x_ff = Dense(inputs.shape[-1])(x_ff)\n","    x = Add()([x, x_ff])  # Residual connection\n","    x = LayerNormalization(epsilon=1e-6)(x)\n","\n","    return x\n","\n","\n","# Model Definition\n","def cnn_lstm_attention_transformer_model(input_shape, num_cnn_filters=16, kernel_size=2, lstm_units=32, gru_units=32, head_size=16, num_heads=2, ff_dim=16, num_transformer_blocks=3, dropout=0.2, weight_decay=1e-6):\n","    inputs = Input(shape=input_shape)\n","    \n","    x = Conv1D(filters=num_cnn_filters, kernel_size=kernel_size, padding='same', activation='relu', kernel_regularizer=l2(weight_decay))(inputs)\n","    x = MaxPooling1D(pool_size=2)(x) \n","    x = Conv1D(filters=num_cnn_filters, kernel_size=kernel_size, padding='same', activation='relu', kernel_regularizer=l2(weight_decay))(inputs)\n","    x = MaxPooling1D(pool_size=2)(x)\n","    x = Conv1D(filters=num_cnn_filters * 2, kernel_size=kernel_size, padding='same', activation='relu', kernel_regularizer=l2(weight_decay))(x)\n","    x = MaxPooling1D(pool_size=2)(x)\n","    x = Dropout(dropout)(x)  # Dropout after pooling\n","    \n","    x = LayerNormalization()(x)\n","\n","    x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout, kernel_regularizer=l2(weight_decay)))(x)\n","    x = LeakyReLU()(x)\n","    x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout, kernel_regularizer=l2(weight_decay)))(x)\n","    x = LeakyReLU()(x)\n","    x = Bidirectional(LSTM(lstm_units // 2, return_sequences=True, dropout=dropout, recurrent_dropout=dropout, kernel_regularizer=l2(weight_decay)))(x)\n","    x = LeakyReLU()(x)\n","    x = LayerNormalization()(x)\n","\n","    x = Bidirectional(GRU(gru_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout, kernel_regularizer=l2(weight_decay)))(x)\n","    x = LeakyReLU()(x)\n","    x = Bidirectional(GRU(gru_units, return_sequences=True, dropout=dropout, recurrent_dropout=dropout, kernel_regularizer=l2(weight_decay)))(x)\n","    x = LeakyReLU()(x)\n","    x = Bidirectional(GRU(gru_units // 2, return_sequences=True, dropout=dropout, recurrent_dropout=dropout, kernel_regularizer=l2(weight_decay)))(x)\n","    x = LeakyReLU()(x)\n","    x = LayerNormalization()(x)\n","\n","    # Attention Mechanism (optional)\n","    attention = Attention()([x, x])  # Self-attention over the LSTM/GRU output\n","    x = Add()([x, attention])  # Add residual connection after attention\n","    \n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n","\n","    x = GlobalAveragePooling1D()(x)\n","    x = Dropout(dropout)(x)  # Dropout after pooling layer\n","    outputs = Dense(1, kernel_regularizer=l2(weight_decay))(x)\n","    \n","    model = Model(inputs, outputs)\n","    model.compile(optimizer='adam', loss= tf.losses.Huber(delta=1.5), metrics=['mse', 'mae'])\n","    \n","    return model\n","\n","# Function to create lag features\n","def create_features(df):\n","    df['Hour'] = df.index.hour\n","    df['Day_of_Week'] = df.index.dayofweek\n","    df['Month'] = df.index.month\n","    # Create lag features for consumption and production\n","    df['Lag_1'] = df['Consumption(W)'].shift(1)\n","    df['Lag_60'] = df['Consumption(W)'].shift(60)  # Lag by 1 hour (60 minutes)\n","    df['Lag_1440'] = df['Consumption(W)'].shift(1440)  # Lag by 1 day (1440 minutes)\n","    df['Lag_1_prod'] = df['Production(W)'].shift(1)\n","    df['Lag_60_prod'] = df['Production(W)'].shift(60)\n","    df['Lag_1440_prod'] = df['Production(W)'].shift(1440)\n","    df.dropna(inplace=True)\n","    return df\n","\n","# Function to create sequences for additional features\n","def create_sequences_with_additional_features(dataset, additional_features, window_size=1):\n","    dataX, dataY = [], []\n","    for i in range(len(dataset) - window_size):\n","        seq_X = dataset[i:(i + window_size), :]\n","        seq_features = additional_features[i + window_size - 1]  # Add the corresponding additional feature to each sequence\n","        dataX.append(np.concatenate((seq_X, np.tile(seq_features, (window_size, 1))), axis=1))\n","        dataY.append(dataset[i + window_size, :])\n","    return np.array(dataX), np.array(dataY)\n","\n","# Function to prepare timeseries generators\n","def prepare_timeseries_generator(target, features, window_size=24, batch_size=64):\n","    generator_input = np.concatenate([features, target], axis=1)\n","    generator = TimeseriesGenerator(generator_input, target, length=window_size, batch_size=batch_size)\n","    return generator\n","\n","\n","def print_cpu_usage(phase):\n","    cpu_percent = psutil.cpu_percent(interval=1)\n","    print(f\"CPU usage during {phase}: {cpu_percent}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["RNN_models = {}\n","for name, df in houses.items():\n","    print(f\"Processing house: {name}\")\n","\n","    df = create_features(df)\n","\n","    # Convert columns to numeric\n","    df['Consumption(W)'] = pd.to_numeric(df['Consumption(W)'], errors='coerce')\n","    df['Production(W)'] = pd.to_numeric(df['Production(W)'], errors='coerce')\n","\n","    # Scale data\n","    consumption = df['Consumption(W)'].values.reshape(-1, 1)\n","    production = df['Production(W)'].values.reshape(-1, 1)\n","    additional_features = df[['Hour', 'Day_of_Week', 'Month', 'Lag_1', 'Lag_60', 'Lag_1440', 'Lag_1_prod', 'Lag_60_prod', 'Lag_1440_prod', 'speed', 'dir', 'drybulb', 'cbl', 'soltot', 'rain']]\n","    additional_features = additional_features.apply(pd.to_numeric, errors='coerce')\n","    additional_features = additional_features.fillna(method='ffill').fillna(method='bfill')\n","    additional_features = additional_features.values\n","\n","\n","    split_point = int(len(consumption) * 0.80)\n","\n","    scaler_consumption = RobustScaler()\n","    scaler_production = RobustScaler()\n","    scaler_features = RobustScaler()\n","\n","    train_consumption = scaler_consumption.fit_transform(consumption[:split_point])\n","    test_consumption = scaler_consumption.transform(consumption[split_point:])\n","    train_production = scaler_production.fit_transform(production[:split_point])\n","    test_production = scaler_production.transform(production[split_point:])\n","    train_additional_features = scaler_features.fit_transform(additional_features[:split_point])\n","    test_additional_features = scaler_features.transform(additional_features[split_point:])\n","\n","    window_size = 60\n","    batch_size = 64\n","\n","    # Model for Consumption\n","    print(\"Training consumption model...\")\n","    train_generator_consumption = prepare_timeseries_generator(train_consumption, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_consumption = prepare_timeseries_generator(test_consumption, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    input_shape = (window_size, train_generator_consumption[0][0].shape[2])\n","    consumption_model = create_rnn_model(input_shape=input_shape)\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n","\n","    history_consumption = consumption_model.fit(train_generator_consumption, epochs=100, validation_data=test_generator_consumption, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    # Model for Production\n","    print(\"Training production model...\")\n","    train_generator_production = prepare_timeseries_generator(train_production, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_production = prepare_timeseries_generator(test_production, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    production_model = create_rnn_model_model(input_shape=input_shape)\n","\n","    history_production = production_model.fit(train_generator_production, epochs=100, validation_data=test_generator_production, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    predictions_consumption = consumption_model.predict(test_generator_consumption)\n","    predictions_production = production_model.predict(test_generator_production)\n","\n","    actual_consumption = scaler_consumption.inverse_transform(test_consumption[window_size:])\n","    actual_production = scaler_production.inverse_transform(test_production[window_size:])\n","    predicted_consumption = scaler_consumption.inverse_transform(predictions_consumption)\n","    predicted_production = scaler_production.inverse_transform(predictions_production)\n","\n","    rmse_consumption = mean_squared_error(actual_consumption, predicted_consumption, squared=False)\n","    mae_consumption = mean_absolute_error(actual_consumption, predicted_consumption)\n","    r2_consumption = r2_score(actual_consumption, predicted_consumption)\n","    epsilon = 1e-5\n","    mape_consumption = np.mean(np.abs((actual_consumption - predictions_consumption) / np.maximum(np.abs(actual_consumption), epsilon))) * 100\n","    \n","    print(f\"RMSE (Consumption): {rmse_consumption}\")\n","    print(f\"MAE (Consumption): {mae_consumption}\")\n","    print(f\"RÂ² (Consumption): {r2_consumption}\")\n","    print(f'MAPE (Consumption): {mape_consumption}%')\n","\n","    rmse_production = mean_squared_error(actual_production, predicted_production, squared=False)\n","    mae_production = mean_absolute_error(actual_production, predicted_production)\n","    r2_production = r2_score(actual_production, predicted_production)\n","    mape_consumption = np.mean(np.abs((actual_production - predicted_production) / np.maximum(np.abs(actual_production), epsilon))) * 100\n","\n","    print(f\"RMSE (Production): {rmse_production}\")\n","    print(f\"MAE (Production): {mae_production}\")\n","    print(f\"RÂ² (Production): {r2_production}\")\n","    print(f'MAPE (Production): {mape_production}%')\n","    con_model_weights_path = f\"{name}_2RNN_Consumption_weights.h5\"\n","    consumption_model.save_weights(con_model_weights_path)\n","    prod_model_weights_path = f\"{name}_2RNN_Production_weights.h5\"\n","    production_model.save_weights(prod_model_weights_path)\n","    RNN_models[name] = {\n","        'model': model,\n","        'actual_consumption': actual_consumption,\n","        'predicted_consumption': predicted_consumption,\n","        'rmse_consumption': rmse_production,\n","        'mae_consumption': mae_production,\n","        'r2_consumption': r2_production,\n","        'mape_consumption':  mape_consumption,\n","        'actual_production': actual_production,\n","        'predicted_production': predicted_production,\n","        'rmse_production': rmse_production,\n","        'mae_production': mae_production,\n","        'r2_production': r2_production,\n","        'mape_production':  mape_production,\n","        'con_model': consumption_model, \n","        'prod_model':production_model,\n","        'con_model_weights_path': con_model_weights_path,\n","        'prod_model_weights_path': prod_model_weights_path\n","    }\n","\n","    aligned_dates = df.index[split_point + window_size:split_point + window_size + len(actual_consumption)]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_consumption, label='Actual Consumption')\n","    plt.plot(aligned_dates, predicted_consumption, label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2RNN_House_{name}_full_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for consumption\n","    plt.show()\n","\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_production, label='Actual Production')\n","    plt.plot(aligned_dates, predicted_production, label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2RNN_House_{name}_full_production_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for production\n","    plt.show()\n","\n","\n","    points_in_24h = 1440 \n","    aligned_dates_24h = aligned_dates[:points_in_24h]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_consumption[:points_in_24h], label='Actual Consumption')\n","    plt.plot(aligned_dates_24h, predicted_consumption[:points_in_24h], label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2RNN_House_{name}_24h_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for consumption\n","    plt.show()\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_production[:points_in_24h], label='Actual Production')\n","    plt.plot(aligned_dates_24h, predicted_production[:points_in_24h], label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2RNN_House_{name}_24h_production_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for production\n","    plt.show()\n","\n","    results_df = pd.DataFrame({\n","        'Date': aligned_dates,\n","        'Actual_Consumption': actual_consumption.flatten(), \n","        'Predicted_Consumption': predicted_consumption.flatten(),\n","        'Actual_Production': actual_production.flatten(),\n","        'Predicted_Production': predicted_production.flatten()\n","    })\n","\n","    results_df.to_csv(f\"2RNN_{name}_results.csv\", index=False)\n","    print(f\"Results saved to 2RNN_{name}_results.csv\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["LSTM_models = {}\n","for name, df in houses.items():\n","    print(f\"Processing house: {name}\")\n","\n","    df = create_features(df)\n","\n","    # Convert columns to numeric\n","    df['Consumption(W)'] = pd.to_numeric(df['Consumption(W)'], errors='coerce')\n","    df['Production(W)'] = pd.to_numeric(df['Production(W)'], errors='coerce')\n","\n","    # Scale data\n","    consumption = df['Consumption(W)'].values.reshape(-1, 1)\n","    production = df['Production(W)'].values.reshape(-1, 1)\n","    additional_features = df[['Hour', 'Day_of_Week', 'Month', 'Lag_1', 'Lag_60', 'Lag_1440', 'Lag_1_prod', 'Lag_60_prod', 'Lag_1440_prod', 'speed', 'dir', 'drybulb', 'cbl', 'soltot', 'rain']]\n","    additional_features = additional_features.apply(pd.to_numeric, errors='coerce')\n","    additional_features = additional_features.fillna(method='ffill').fillna(method='bfill')\n","    additional_features = additional_features.values\n","    split_point = int(len(consumption) * 0.80)\n","\n","    scaler_consumption = RobustScaler()\n","    scaler_production = RobustScaler()\n","    scaler_features = RobustScaler()\n","\n","    train_consumption = scaler_consumption.fit_transform(consumption[:split_point])\n","    test_consumption = scaler_consumption.transform(consumption[split_point:])\n","    train_production = scaler_production.fit_transform(production[:split_point])\n","    test_production = scaler_production.transform(production[split_point:])\n","    train_additional_features = scaler_features.fit_transform(additional_features[:split_point])\n","    test_additional_features = scaler_features.transform(additional_features[split_point:])\n","\n","    window_size = 60\n","    batch_size = 64\n","\n","    # Model for Consumption\n","    print(\"Training consumption model...\")\n","    train_generator_consumption = prepare_timeseries_generator(train_consumption, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_consumption = prepare_timeseries_generator(test_consumption, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    input_shape = (window_size, train_generator_consumption[0][0].shape[2])\n","    consumption_model = cnn_lstm_model(input_shape=input_shape)\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n","\n","    history_consumption = consumption_model.fit(train_generator_consumption, epochs=100, validation_data=test_generator_consumption, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    # Model for Production\n","    print(\"Training production model...\")\n","    train_generator_production = prepare_timeseries_generator(train_production, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_production = prepare_timeseries_generator(test_production, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    production_model = create_lstm_model_model(input_shape=input_shape)\n","\n","    history_production = production_model.fit(train_generator_production, epochs=100, validation_data=test_generator_production, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    predictions_consumption = consumption_model.predict(test_generator_consumption)\n","    predictions_production = production_model.predict(test_generator_production)\n","\n","    actual_consumption = scaler_consumption.inverse_transform(test_consumption[window_size:])\n","    actual_production = scaler_production.inverse_transform(test_production[window_size:])\n","    predicted_consumption = scaler_consumption.inverse_transform(predictions_consumption)\n","    predicted_production = scaler_production.inverse_transform(predictions_production)\n","\n","    rmse_consumption = mean_squared_error(actual_consumption, predicted_consumption, squared=False)\n","    mae_consumption = mean_absolute_error(actual_consumption, predicted_consumption)\n","    r2_consumption = r2_score(actual_consumption, predicted_consumption)\n","    epsilon = 1e-5\n","    mape_consumption = np.mean(np.abs((actual_consumption - predictions_consumption) / np.maximum(np.abs(actual_consumption), epsilon))) * 100\n","    \n","    print(f\"RMSE (Consumption): {rmse_consumption}\")\n","    print(f\"MAE (Consumption): {mae_consumption}\")\n","    print(f\"RÂ² (Consumption): {r2_consumption}\")\n","    print(f'MAPE (Consumption): {mape_consumption}%')\n","\n","    rmse_production = mean_squared_error(actual_production, predicted_production, squared=False)\n","    mae_production = mean_absolute_error(actual_production, predicted_production)\n","    r2_production = r2_score(actual_production, predicted_production)\n","    mape_consumption = np.mean(np.abs((actual_production - predicted_production) / np.maximum(np.abs(actual_production), epsilon))) * 100\n","\n","    print(f\"RMSE (Production): {rmse_production}\")\n","    print(f\"MAE (Production): {mae_production}\")\n","    print(f\"RÂ² (Production): {r2_production}\")\n","    print(f'MAPE (Production): {mape_production}%')\n","    con_model_weights_path = f\"{name}_2LSTM_Consumption_weights.h5\"\n","    consumption_model.save_weights(con_model_weights_path)\n","    prod_model_weights_path = f\"{name}_2LSTM_Production_weights.h5\"\n","    production_model.save_weights(prod_model_weights_path)\n","    LSTM_models[name] = {\n","        'model': model,\n","        'actual_consumption': actual_consumption,\n","        'predicted_consumption': predicted_consumption,\n","        'rmse_consumption': rmse_production,\n","        'mae_consumption': mae_production,\n","        'r2_consumption': r2_production,\n","        'mape_consumption':  mape_consumption,\n","        'actual_production': actual_production,\n","        'predicted_production': predicted_production,\n","        'rmse_production': rmse_production,\n","        'mae_production': mae_production,\n","        'r2_production': r2_production,\n","        'mape_production':  mape_production,\n","        'con_model': consumption_model, \n","        'prod_model':production_model,\n","        'con_model_weights_path': con_model_weights_path,\n","        'prod_model_weights_path': prod_model_weights_path\n","    }\n","\n","    aligned_dates = df.index[split_point + window_size:split_point + window_size + len(actual_consumption)]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_consumption, label='Actual Consumption')\n","    plt.plot(aligned_dates, predicted_consumption, label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2LSTM_House_{name}_full_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for consumption\n","    plt.show()\n","\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_production, label='Actual Production')\n","    plt.plot(aligned_dates, predicted_production, label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2LSTM_House_{name}_full_production_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for production\n","    plt.show()\n","\n","\n","    points_in_24h = 1440 \n","    aligned_dates_24h = aligned_dates[:points_in_24h]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_consumption[:points_in_24h], label='Actual Consumption')\n","    plt.plot(aligned_dates_24h, predicted_consumption[:points_in_24h], label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2LSTM_House_{name}_24h_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for consumption\n","    plt.show()\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_production[:points_in_24h], label='Actual Production')\n","    plt.plot(aligned_dates_24h, predicted_production[:points_in_24h], label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2LSTM_House_{name}_24h_production_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for production\n","    plt.show()\n","\n","    results_df = pd.DataFrame({\n","        'Date': aligned_dates,\n","        'Actual_Consumption': actual_consumption.flatten(), \n","        'Predicted_Consumption': predicted_consumption.flatten(),\n","        'Actual_Production': actual_production.flatten(),\n","        'Predicted_Production': predicted_production.flatten()\n","    })\n","\n","    results_df.to_csv(f\"2LSTM_{name}_results.csv\", index=False)\n","    print(f\"Results saved to 2LSTM_{name}_results.csv\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["BiLSTM_models = {}\n","for name, df in houses.items():\n","    print(f\"Processing house: {name}\")\n","\n","    df = create_features(df)\n","\n","    # Convert columns to numeric\n","    df['Consumption(W)'] = pd.to_numeric(df['Consumption(W)'], errors='coerce')\n","    df['Production(W)'] = pd.to_numeric(df['Production(W)'], errors='coerce')\n","\n","    # Scale data\n","    consumption = df['Consumption(W)'].values.reshape(-1, 1)\n","    production = df['Production(W)'].values.reshape(-1, 1)\n","    additional_features = df[['Hour', 'Day_of_Week', 'Month', 'Lag_1', 'Lag_60', 'Lag_1440', 'Lag_1_prod', 'Lag_60_prod', 'Lag_1440_prod', 'speed', 'dir', 'drybulb', 'cbl', 'soltot', 'rain']]\n","    additional_features = additional_features.apply(pd.to_numeric, errors='coerce')\n","    additional_features = additional_features.fillna(method='ffill').fillna(method='bfill')\n","    additional_features = additional_features.values\n","    split_point = int(len(consumption) * 0.80)\n","\n","    scaler_consumption = RobustScaler()\n","    scaler_production = RobustScaler()\n","    scaler_features = RobustScaler()\n","\n","    train_consumption = scaler_consumption.fit_transform(consumption[:split_point])\n","    test_consumption = scaler_consumption.transform(consumption[split_point:])\n","    train_production = scaler_production.fit_transform(production[:split_point])\n","    test_production = scaler_production.transform(production[split_point:])\n","    train_additional_features = scaler_features.fit_transform(additional_features[:split_point])\n","    test_additional_features = scaler_features.transform(additional_features[split_point:])\n","\n","    window_size = 60\n","    batch_size = 64\n","\n","    # Model for Consumption\n","    print(\"Training consumption model...\")\n","    train_generator_consumption = prepare_timeseries_generator(train_consumption, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_consumption = prepare_timeseries_generator(test_consumption, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    input_shape = (window_size, train_generator_consumption[0][0].shape[2])\n","    consumption_model = create_bilstm_model(input_shape=input_shape)\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n","\n","    history_consumption = consumption_model.fit(train_generator_consumption, epochs=100, validation_data=test_generator_consumption, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    # Model for Production\n","    print(\"Training production model...\")\n","    train_generator_production = prepare_timeseries_generator(train_production, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_production = prepare_timeseries_generator(test_production, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    production_model = create_bilstm_model_model(input_shape=input_shape)\n","\n","    history_production = production_model.fit(train_generator_production, epochs=100, validation_data=test_generator_production, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    predictions_consumption = consumption_model.predict(test_generator_consumption)\n","    predictions_production = production_model.predict(test_generator_production)\n","\n","    actual_consumption = scaler_consumption.inverse_transform(test_consumption[window_size:])\n","    actual_production = scaler_production.inverse_transform(test_production[window_size:])\n","    predicted_consumption = scaler_consumption.inverse_transform(predictions_consumption)\n","    predicted_production = scaler_production.inverse_transform(predictions_production)\n","\n","    rmse_consumption = mean_squared_error(actual_consumption, predicted_consumption, squared=False)\n","    mae_consumption = mean_absolute_error(actual_consumption, predicted_consumption)\n","    r2_consumption = r2_score(actual_consumption, predicted_consumption)\n","    epsilon = 1e-5\n","    mape_consumption = np.mean(np.abs((actual_consumption - predictions_consumption) / np.maximum(np.abs(actual_consumption), epsilon))) * 100\n","    \n","    print(f\"RMSE (Consumption): {rmse_consumption}\")\n","    print(f\"MAE (Consumption): {mae_consumption}\")\n","    print(f\"RÂ² (Consumption): {r2_consumption}\")\n","    print(f'MAPE (Consumption): {mape_consumption}%')\n","\n","    rmse_production = mean_squared_error(actual_production, predicted_production, squared=False)\n","    mae_production = mean_absolute_error(actual_production, predicted_production)\n","    r2_production = r2_score(actual_production, predicted_production)\n","    mape_consumption = np.mean(np.abs((actual_production - predicted_production) / np.maximum(np.abs(actual_production), epsilon))) * 100\n","\n","    print(f\"RMSE (Production): {rmse_production}\")\n","    print(f\"MAE (Production): {mae_production}\")\n","    print(f\"RÂ² (Production): {r2_production}\")\n","    print(f'MAPE (Production): {mape_production}%')\n","    con_model_weights_path = f\"{name}_2BiLSTM_Consumption_weights.h5\"\n","    consumption_model.save_weights(con_model_weights_path)\n","    prod_model_weights_path = f\"{name}_2BiLSTM_Production_weights.h5\"\n","    production_model.save_weights(prod_model_weights_path)\n","    BiLSTM_models[name] = {\n","        'model': model,\n","        'actual_consumption': actual_consumption,\n","        'predicted_consumption': predicted_consumption,\n","        'rmse_consumption': rmse_production,\n","        'mae_consumption': mae_production,\n","        'r2_consumption': r2_production,\n","        'mape_consumption':  mape_consumption,\n","        'actual_production': actual_production,\n","        'predicted_production': predicted_production,\n","        'rmse_production': rmse_production,\n","        'mae_production': mae_production,\n","        'r2_production': r2_production,\n","        'mape_production':  mape_production,\n","        'con_model': consumption_model, \n","        'prod_model':production_model,\n","        'con_model_weights_path': con_model_weights_path,\n","        'prod_model_weights_path': prod_model_weights_path\n","    }\n","\n","    aligned_dates = df.index[split_point + window_size:split_point + window_size + len(actual_consumption)]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_consumption, label='Actual Consumption')\n","    plt.plot(aligned_dates, predicted_consumption, label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2BiLSTM_House_{name}_full_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for consumption\n","    plt.show()\n","\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_production, label='Actual Production')\n","    plt.plot(aligned_dates, predicted_production, label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2BiLSTM_House_{name}_full_production_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for production\n","    plt.show()\n","\n","\n","    points_in_24h = 1440 \n","    aligned_dates_24h = aligned_dates[:points_in_24h]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_consumption[:points_in_24h], label='Actual Consumption')\n","    plt.plot(aligned_dates_24h, predicted_consumption[:points_in_24h], label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2BiLSTM_House_{name}_24h_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for consumption\n","    plt.show()\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_production[:points_in_24h], label='Actual Production')\n","    plt.plot(aligned_dates_24h, predicted_production[:points_in_24h], label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2BiLSTM_House_{name}_24h_production_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for production\n","    plt.show()\n","\n","    results_df = pd.DataFrame({\n","        'Date': aligned_dates,\n","        'Actual_Consumption': actual_consumption.flatten(), \n","        'Predicted_Consumption': predicted_consumption.flatten(),\n","        'Actual_Production': actual_production.flatten(),\n","        'Predicted_Production': predicted_production.flatten()\n","    })\n","\n","    results_df.to_csv(f\"2BiLSTM_{name}_results.csv\", index=False)\n","    print(f\"Results saved to 2BiLSTM_{name}_results.csv\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["GRU_models = {}\n","for name, df in houses.items():\n","    print(f\"Processing house: {name}\")\n","\n","    df = create_features(df)\n","\n","    # Convert columns to numeric\n","    df['Consumption(W)'] = pd.to_numeric(df['Consumption(W)'], errors='coerce')\n","    df['Production(W)'] = pd.to_numeric(df['Production(W)'], errors='coerce')\n","\n","    # Scale data\n","    consumption = df['Consumption(W)'].values.reshape(-1, 1)\n","    production = df['Production(W)'].values.reshape(-1, 1)\n","    additional_features = df[['Hour', 'Day_of_Week', 'Month', 'Lag_1', 'Lag_60', 'Lag_1440', 'Lag_1_prod', 'Lag_60_prod', 'Lag_1440_prod', 'speed', 'dir', 'drybulb', 'cbl', 'soltot', 'rain']]\n","    additional_features = additional_features.apply(pd.to_numeric, errors='coerce')\n","    additional_features = additional_features.fillna(method='ffill').fillna(method='bfill')\n","    additional_features = additional_features.values\n","    split_point = int(len(consumption) * 0.80)\n","\n","    scaler_consumption = RobustScaler()\n","    scaler_production = RobustScaler()\n","    scaler_features = RobustScaler()\n","\n","    train_consumption = scaler_consumption.fit_transform(consumption[:split_point])\n","    test_consumption = scaler_consumption.transform(consumption[split_point:])\n","    train_production = scaler_production.fit_transform(production[:split_point])\n","    test_production = scaler_production.transform(production[split_point:])\n","    train_additional_features = scaler_features.fit_transform(additional_features[:split_point])\n","    test_additional_features = scaler_features.transform(additional_features[split_point:])\n","\n","    window_size = 60\n","    batch_size = 64\n","\n","    # Model for Consumption\n","    print(\"Training consumption model...\")\n","    train_generator_consumption = prepare_timeseries_generator(train_consumption, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_consumption = prepare_timeseries_generator(test_consumption, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    input_shape = (window_size, train_generator_consumption[0][0].shape[2])\n","    consumption_model = create_gru_model(input_shape=input_shape)\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n","\n","    history_consumption = consumption_model.fit(train_generator_consumption, epochs=100, validation_data=test_generator_consumption, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    # Model for Production\n","    print(\"Training production model...\")\n","    train_generator_production = prepare_timeseries_generator(train_production, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_production = prepare_timeseries_generator(test_production, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    production_model = create_gru_model_model(input_shape=input_shape)\n","\n","    history_production = production_model.fit(train_generator_production, epochs=100, validation_data=test_generator_production, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    predictions_consumption = consumption_model.predict(test_generator_consumption)\n","    predictions_production = production_model.predict(test_generator_production)\n","\n","    actual_consumption = scaler_consumption.inverse_transform(test_consumption[window_size:])\n","    actual_production = scaler_production.inverse_transform(test_production[window_size:])\n","    predicted_consumption = scaler_consumption.inverse_transform(predictions_consumption)\n","    predicted_production = scaler_production.inverse_transform(predictions_production)\n","\n","    rmse_consumption = mean_squared_error(actual_consumption, predicted_consumption, squared=False)\n","    mae_consumption = mean_absolute_error(actual_consumption, predicted_consumption)\n","    r2_consumption = r2_score(actual_consumption, predicted_consumption)\n","    epsilon = 1e-5\n","    mape_consumption = np.mean(np.abs((actual_consumption - predictions_consumption) / np.maximum(np.abs(actual_consumption), epsilon))) * 100\n","    \n","    print(f\"RMSE (Consumption): {rmse_consumption}\")\n","    print(f\"MAE (Consumption): {mae_consumption}\")\n","    print(f\"RÂ² (Consumption): {r2_consumption}\")\n","    print(f'MAPE (Consumption): {mape_consumption}%')\n","\n","    rmse_production = mean_squared_error(actual_production, predicted_production, squared=False)\n","    mae_production = mean_absolute_error(actual_production, predicted_production)\n","    r2_production = r2_score(actual_production, predicted_production)\n","    mape_consumption = np.mean(np.abs((actual_production - predicted_production) / np.maximum(np.abs(actual_production), epsilon))) * 100\n","\n","    print(f\"RMSE (Production): {rmse_production}\")\n","    print(f\"MAE (Production): {mae_production}\")\n","    print(f\"RÂ² (Production): {r2_production}\")\n","    print(f'MAPE (Production): {mape_production}%')\n","    con_model_weights_path = f\"{name}_2GRU_Consumption_weights.h5\"\n","    consumption_model.save_weights(con_model_weights_path)\n","    prod_model_weights_path = f\"{name}_2GRU_Production_weights.h5\"\n","    production_model.save_weights(prod_model_weights_path)\n","    GRU_models[name] = {\n","        'model': model,\n","        'actual_consumption': actual_consumption,\n","        'predicted_consumption': predicted_consumption,\n","        'rmse_consumption': rmse_production,\n","        'mae_consumption': mae_production,\n","        'r2_consumption': r2_production,\n","        'mape_consumption':  mape_consumption,\n","        'actual_production': actual_production,\n","        'predicted_production': predicted_production,\n","        'rmse_production': rmse_production,\n","        'mae_production': mae_production,\n","        'r2_production': r2_production,\n","        'mape_production':  mape_production,\n","        'con_model': consumption_model, \n","        'prod_model':production_model,\n","        'con_model_weights_path': con_model_weights_path,\n","        'prod_model_weights_path': prod_model_weights_path\n","    }\n","\n","    aligned_dates = df.index[split_point + window_size:split_point + window_size + len(actual_consumption)]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_consumption, label='Actual Consumption')\n","    plt.plot(aligned_dates, predicted_consumption, label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2GRU_House_{name}_full_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for consumption\n","    plt.show()\n","\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_production, label='Actual Production')\n","    plt.plot(aligned_dates, predicted_production, label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2GRU_House_{name}_full_production_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for production\n","    plt.show()\n","\n","\n","    points_in_24h = 1440 \n","    aligned_dates_24h = aligned_dates[:points_in_24h]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_consumption[:points_in_24h], label='Actual Consumption')\n","    plt.plot(aligned_dates_24h, predicted_consumption[:points_in_24h], label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2GRU_House_{name}_24h_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for consumption\n","    plt.show()\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_production[:points_in_24h], label='Actual Production')\n","    plt.plot(aligned_dates_24h, predicted_production[:points_in_24h], label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2GRU_House_{name}_24h_production_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for production\n","    plt.show()\n","\n","    results_df = pd.DataFrame({\n","        'Date': aligned_dates,\n","        'Actual_Consumption': actual_consumption.flatten(), \n","        'Predicted_Consumption': predicted_consumption.flatten(),\n","        'Actual_Production': actual_production.flatten(),\n","        'Predicted_Production': predicted_production.flatten()\n","    })\n","\n","    results_df.to_csv(f\"2GRU_{name}_results.csv\", index=False)\n","    print(f\"Results saved to 2GRU_{name}_results.csv\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Transformer_models = {}\n","for name, df in houses.items():\n","    print(f\"Processing house: {name}\")\n","\n","    df = create_features(df)\n","\n","    # Convert columns to numeric\n","    df['Consumption(W)'] = pd.to_numeric(df['Consumption(W)'], errors='coerce')\n","    df['Production(W)'] = pd.to_numeric(df['Production(W)'], errors='coerce')\n","\n","    # Scale data\n","    consumption = df['Consumption(W)'].values.reshape(-1, 1)\n","    production = df['Production(W)'].values.reshape(-1, 1)\n","    additional_features = df[['Hour', 'Day_of_Week', 'Month', 'Lag_1', 'Lag_60', 'Lag_1440', 'Lag_1_prod', 'Lag_60_prod', 'Lag_1440_prod', 'speed', 'dir', 'drybulb', 'cbl', 'soltot', 'rain']]\n","    additional_features = additional_features.apply(pd.to_numeric, errors='coerce')\n","    additional_features = additional_features.fillna(method='ffill').fillna(method='bfill')\n","    additional_features = additional_features.values\n","    split_point = int(len(consumption) * 0.80)\n","\n","    scaler_consumption = RobustScaler()\n","    scaler_production = RobustScaler()\n","    scaler_features = RobustScaler()\n","\n","    train_consumption = scaler_consumption.fit_transform(consumption[:split_point])\n","    test_consumption = scaler_consumption.transform(consumption[split_point:])\n","    train_production = scaler_production.fit_transform(production[:split_point])\n","    test_production = scaler_production.transform(production[split_point:])\n","    train_additional_features = scaler_features.fit_transform(additional_features[:split_point])\n","    test_additional_features = scaler_features.transform(additional_features[split_point:])\n","\n","    window_size = 60\n","    batch_size = 64\n","\n","    # Model for Consumption\n","    print(\"Training consumption model...\")\n","    train_generator_consumption = prepare_timeseries_generator(train_consumption, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_consumption = prepare_timeseries_generator(test_consumption, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    input_shape = (window_size, train_generator_consumption[0][0].shape[2])\n","    consumption_model = cnn_lstm_attention_transformer_model(input_shape=input_shape, dropout=0.2, weight_decay=1e-7)\n","\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-7)\n","\n","    history_consumption = consumption_model.fit(train_generator_consumption, epochs=100, validation_data=test_generator_consumption, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    # Model for Production\n","    print(\"Training production model...\")\n","    train_generator_production = prepare_timeseries_generator(train_production, train_additional_features, window_size=window_size, batch_size=batch_size)\n","    test_generator_production = prepare_timeseries_generator(test_production, test_additional_features, window_size=window_size, batch_size=batch_size)\n","\n","    production_model = cnn_lstm_attention_transformer_model(input_shape=input_shape, dropout=0.2, weight_decay=1e-7)\n","\n","    history_production = production_model.fit(train_generator_production, epochs=100, validation_data=test_generator_production, callbacks=[early_stopping, reduce_lr], batch_size=batch_size)\n","\n","    predictions_consumption = consumption_model.predict(test_generator_consumption)\n","    predictions_production = production_model.predict(test_generator_production)\n","\n","    actual_consumption = scaler_consumption.inverse_transform(test_consumption[window_size:])\n","    actual_production = scaler_production.inverse_transform(test_production[window_size:])\n","    predicted_consumption = scaler_consumption.inverse_transform(predictions_consumption)\n","    predicted_production = scaler_production.inverse_transform(predictions_production)\n","\n","    rmse_consumption = mean_squared_error(actual_consumption, predicted_consumption, squared=False)\n","    mae_consumption = mean_absolute_error(actual_consumption, predicted_consumption)\n","    r2_consumption = r2_score(actual_consumption, predicted_consumption)\n","    epsilon = 1e-5\n","    mape_consumption = np.mean(np.abs((actual_consumption - predictions_consumption) / np.maximum(np.abs(actual_consumption), epsilon))) * 100\n","    \n","    print(f\"RMSE (Consumption): {rmse_consumption}\")\n","    print(f\"MAE (Consumption): {mae_consumption}\")\n","    print(f\"RÂ² (Consumption): {r2_consumption}\")\n","    print(f'MAPE (Consumption): {mape_consumption}%')\n","\n","    rmse_production = mean_squared_error(actual_production, predicted_production, squared=False)\n","    mae_production = mean_absolute_error(actual_production, predicted_production)\n","    r2_production = r2_score(actual_production, predicted_production)\n","    mape_consumption = np.mean(np.abs((actual_production - predicted_production) / np.maximum(np.abs(actual_production), epsilon))) * 100\n","\n","    print(f\"RMSE (Production): {rmse_production}\")\n","    print(f\"MAE (Production): {mae_production}\")\n","    print(f\"RÂ² (Production): {r2_production}\")\n","    print(f'MAPE (Production): {mape_production}%')\n","    con_model_weights_path = f\"{name}_2Hybrid_Consumption_weights.h5\"\n","    consumption_model.save_weights(con_model_weights_path)\n","    prod_model_weights_path = f\"{name}_2Hybrid_Production_weights.h5\"\n","    production_model.save_weights(prod_model_weights_path)\n","    Transformer_models[name] = {\n","        'model': model,\n","        'actual_consumption': actual_consumption,\n","        'predicted_consumption': predicted_consumption,\n","        'rmse_consumption': rmse_production,\n","        'mae_consumption': mae_production,\n","        'r2_consumption': r2_production,\n","        'mape_consumption':  mape_consumption,\n","        'actual_production': actual_production,\n","        'predicted_production': predicted_production,\n","        'rmse_production': rmse_production,\n","        'mae_production': mae_production,\n","        'r2_production': r2_production,\n","        'mape_production':  mape_production,\n","        'con_model': consumption_model, \n","        'prod_model':production_model,\n","        'con_model_weights_path': con_model_weights_path,\n","        'prod_model_weights_path': prod_model_weights_path\n","    }\n","\n","    aligned_dates = df.index[split_point + window_size:split_point + window_size + len(actual_consumption)]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_consumption, label='Actual Consumption')\n","    plt.plot(aligned_dates, predicted_consumption, label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2Hybrid_House_{name}_full_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for consumption\n","    plt.show()\n","\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates, actual_production, label='Actual Production')\n","    plt.plot(aligned_dates, predicted_production, label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name}')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2Hybrid_House_{name}_full_production_results.jpeg\", format='jpeg', dpi=300)  # Save the full plot for production\n","    plt.show()\n","\n","\n","    points_in_24h = 1440 \n","    aligned_dates_24h = aligned_dates[:points_in_24h]\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_consumption[:points_in_24h], label='Actual Consumption')\n","    plt.plot(aligned_dates_24h, predicted_consumption[:points_in_24h], label='Predicted Consumption', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Consumption (W)')\n","    plt.title(f'Actual vs Predicted Consumption for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2Hybrid_House_{name}_24h_consumption_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for consumption\n","    plt.show()\n","\n","    plt.figure(figsize=(20, 10))\n","    plt.plot(aligned_dates_24h, actual_production[:points_in_24h], label='Actual Production')\n","    plt.plot(aligned_dates_24h, predicted_production[:points_in_24h], label='Predicted Production', linestyle='--')\n","    plt.xlabel('Time')\n","    plt.ylabel('Energy Production (W)')\n","    plt.title(f'Actual vs Predicted Production for {name} - First 24 Hours')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig(f\"2Hybrid_House_{name}_24h_production_results.jpeg\", format='jpeg', dpi=300)  # Save the 24-hour plot for production\n","    plt.show()\n","\n","    results_df = pd.DataFrame({\n","        'Date': aligned_dates,\n","        'Actual_Consumption': actual_consumption.flatten(), \n","        'Predicted_Consumption': predicted_consumption.flatten(),\n","        'Actual_Production': actual_production.flatten(),\n","        'Predicted_Production': predicted_production.flatten()\n","    })\n","\n","    results_df.to_csv(f\"2Hybrid_{name}_results.csv\", index=False)\n","    print(f\"Results saved to 2Hybrid_{name}_results.csv\")\n","    "]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
